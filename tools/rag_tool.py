# tools/rag_tool.py

from langchain_core.tools import tool
from langchain_community.document_loaders import PyMuPDFLoader  # âœ… ë¹ ë¥´ê³  ì •í™•í•œ PDF ë¡œë”
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings          # âœ… ìµœì‹  ê²½ë¡œ
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate                # âœ… ìµœì‹  import
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from typing import Dict
import os
import sys
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# ê²½ë¡œ ì„¤ì •
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import settings
from utils.logger import logger


# ============================================
# ğŸ“‚ ê³ ì • ë¬¸ì„œ ê²½ë¡œ
# ============================================
FIXED_DOCUMENTS = [
    "data/rag_documents/20250128_GPAI_GenAI_FoW_report_final_VOECD.pdf",
    "data/rag_documents/wpiea2025076-print-pdf.pdf"
]


# ============================================
# ğŸŒ í•œê¸€ ì§ˆì˜ â†’ ì˜ì–´ ë³€í™˜
# ============================================
def translate_query_to_english(query: str) -> str:
    """í•œê¸€ ì§ˆì˜ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­ (OpenAI LLM ì‚¬ìš©)"""
    try:
        llm_translator = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        )
        prompt = f"Translate the following Korean text into English, preserving technical and analytical meaning:\n\n{query}"
        response = llm_translator.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        logger.error(f"âš ï¸ ì§ˆì˜ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ìœ¼ë¡œ ì§„í–‰): {e}")
        return query


# ============================================
# ğŸ¤– RAG ë¶„ì„ ë„êµ¬ (LangChain 1.x í˜¸í™˜)
# ============================================
@tool
def analyze_with_fixed_rag(query: str, max_pages_per_doc: int = 35) -> Dict:
    """
    ê³ ì •ëœ PDF ë¬¸ì„œ 2ê°œë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (í•œê¸€ ì§ˆì˜ ì§€ì› + ì˜ì–´ ì„ë² ë”© + í•œêµ­ì–´ ì¶œë ¥)
    """
    logger.info("ğŸ“š Fixed RAG ë¶„ì„ ì‹œì‘")
    logger.info(f"   Query: {query[:100]}...")

    all_documents = []
    loaded_docs = []

    # 1ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ
    for doc_path in FIXED_DOCUMENTS:
        if not os.path.exists(doc_path):
            logger.warning(f"   âš ï¸ ë¬¸ì„œ ì—†ìŒ: {doc_path}")
            continue
        try:
            logger.info(f"   ğŸ“„ ë¡œë”©: {os.path.basename(doc_path)}")

            loader = PyMuPDFLoader(doc_path)
            pages = loader.load()

            if len(pages) > max_pages_per_doc:
                logger.info(f"      â†’ {len(pages)}í˜ì´ì§€ ì¤‘ {max_pages_per_doc}í˜ì´ì§€ë§Œ ì‚¬ìš©")
                pages = pages[:max_pages_per_doc]
            else:
                logger.info(f"      â†’ {len(pages)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

            all_documents.extend(pages)
            loaded_docs.append(os.path.basename(doc_path))

        except Exception as e:
            logger.error(f"   âœ— {doc_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    # ë¬¸ì„œ ì—†ì„ ê²½ìš° ì¢…ë£Œ
    if not all_documents:
        logger.error("   âŒ ë¡œë“œëœ ë¬¸ì„œ ì—†ìŒ")
        return {
            "answer": "RAG ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. data/rag_documents/ í´ë”ì— PDF íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.",
            "sources": [],
            "loaded_documents": [],
            "error": True,
        }

    # 2ï¸âƒ£ í…ìŠ¤íŠ¸ ë¶„í• 
    logger.info(f"   ğŸ“ {len(all_documents)}ê°œ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    splits = text_splitter.split_documents(all_documents)
    logger.info(f"      â†’ {len(splits)}ê°œ ì²­í¬ ìƒì„±")

    # 3ï¸âƒ£ ì„ë² ë”© (Hugging Face ë¬´ë£Œ ëª¨ë¸)
    logger.info("   ğŸ”¢ ë²¡í„° ì„ë² ë”© ì¤‘ (Hugging Face ëª¨ë¸ ì‚¬ìš©)...")
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"  # âœ… ì˜ì–´ ë¬¸ì„œìš©
        )

        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings
        )

        logger.info("      â†’ ì„ë² ë”© ì™„ë£Œ")

    except Exception as e:
        logger.error(f"   âŒ ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return {
            "answer": f"ë²¡í„° ì„ë² ë”© ì‹¤íŒ¨: {str(e)}",
            "sources": [],
            "loaded_documents": loaded_docs,
            "error": True,
        }

    # 4ï¸âƒ£ í•œê¸€ ì§ˆì˜ â†’ ì˜ì–´ ë³€í™˜
    if not query.isascii():
        logger.info("ğŸŒ í•œê¸€ ì§ˆì˜ë¥¼ ì˜ì–´ë¡œ ë³€í™˜ ì¤‘...")
        query_en = translate_query_to_english(query)
        logger.info(f"   ë²ˆì—­ ì™„ë£Œ â†’ {query_en[:120]}...")
    else:
        query_en = query

    # 5ï¸âƒ£ LLM ì´ˆê¸°í™”
    logger.info("   ğŸ¤– LLM ë¶„ì„ ì‹¤í–‰ ì¤‘ (OpenAI LLM ì‚¬ìš©)...")
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        )

        # 6ï¸âƒ£ í•œêµ­ì–´ ë‹µë³€ìš© í”„ë¡¬í”„íŠ¸
        prompt = PromptTemplate(
            template=(
                "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ë¶„ì„ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n"
                "ì§ˆë¬¸:\n{question}\n\n"
                "ì»¨í…ìŠ¤íŠ¸:\n{context}\n"
            ),
            input_variables=["question", "context"]
        )

        # 7ï¸âƒ£ Retriever ì¤€ë¹„
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # 8ï¸âƒ£ ë¬¸ì„œ í¬ë§·í„°
        def format_docs(docs):
            return "\n\n".join(d.page_content for d in docs)

        # 9ï¸âƒ£ LCEL ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸
        rag_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough(),  # query_en ê·¸ëŒ€ë¡œ ì „ë‹¬
            }
            | prompt
            | llm
            | StrOutputParser()
        )

        # 10ï¸âƒ£ ì˜ì–´ ì§ˆì˜ ì‹¤í–‰
        answer_ko = rag_chain.invoke(query_en)

        # 11ï¸âƒ£ ì¶œì²˜ ì •ë¦¬
        top_docs = retriever.invoke(query_en)[:3]
        sources = []
        for doc in top_docs:
            sources.append({
                "content": doc.page_content[:300],
                "page": doc.metadata.get("page", 0) + 1,
                "source": os.path.basename(doc.metadata.get("source", "unknown")),
            })

        logger.info("   âœ… RAG ë¶„ì„ ì™„ë£Œ")
        logger.info(f"      ë‹µë³€ ê¸¸ì´: {len(answer_ko)}ì")

        return {
            "answer": answer_ko,
            "sources": sources,
            "loaded_documents": loaded_docs,
            "num_pages": len(all_documents),
            "num_chunks": len(splits),
            "error": False,
        }

    except Exception as e:
        logger.error(f"   âŒ RAG ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {
            "answer": f"RAG ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
            "sources": [],
            "loaded_documents": loaded_docs,
            "error": True,
        }
