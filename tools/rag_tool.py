# tools/rag_tool.py (ê°œì„  ë²„ì „)
from langchain_core.tools import tool
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from typing import Dict
import os
import sys
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import settings
from utils.logger import logger

# ============================================
# ğŸ“‚ ë²¡í„° ì €ì¥ì†Œ ê²½ë¡œ
# ============================================
VECTORSTORE_DIR = "data/vectorstore"

# ============================================
# ğŸ”„ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹±)
# ============================================
_vectorstore_cache = None  # âœ… ì „ì—­ ìºì‹œ

def get_vectorstore():
    """ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹±)"""
    global _vectorstore_cache
    
    # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    if _vectorstore_cache is not None:
        return _vectorstore_cache
    
    # ë²¡í„° ì €ì¥ì†Œ ì¡´ì¬ í™•ì¸
    if not os.path.exists(VECTORSTORE_DIR):
        logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ì—†ìŒ: {VECTORSTORE_DIR}")
        logger.error(f"   ë¨¼ì € 'python scripts/build_vectorstore.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”!")
        return None
    
    try:
        logger.info(f"ğŸ“‚ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì¤‘: {VECTORSTORE_DIR}")
        
        embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # ì €ì¥ëœ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
        _vectorstore_cache = Chroma(
            persist_directory=VECTORSTORE_DIR,
            embedding_function=embeddings
        )
        
        logger.info(f"   âœ… ë¡œë“œ ì™„ë£Œ (0.1ì´ˆ)")
        return _vectorstore_cache
        
    except Exception as e:
        logger.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# ============================================
# ğŸŒ í•œê¸€ ì§ˆì˜ â†’ ì˜ì–´ ë³€í™˜
# ============================================
def translate_query_to_english(query: str) -> str:
    """í•œê¸€ ì§ˆì˜ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­"""
    try:
        llm_translator = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        )
        prompt = f"Translate the following Korean text into English, preserving technical and analytical meaning:\n\n{query}"
        response = llm_translator.invoke(prompt)
        return response.content.strip()
    except Exception as e:
        logger.error(f"âš ï¸ ì§ˆì˜ ë²ˆì—­ ì‹¤íŒ¨ (ì›ë¬¸ìœ¼ë¡œ ì§„í–‰): {e}")
        return query

# ============================================
# ğŸ¤– RAG ë¶„ì„ ë„êµ¬ (ìµœì í™” ë²„ì „)
# ============================================
@tool
def analyze_with_fixed_rag(query: str) -> Dict:
    """
    ì‚¬ì „ ìƒì„±ëœ ë²¡í„° ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ RAG ë¶„ì„ ìˆ˜í–‰
    (ë§¤ìš° ë¹ ë¦„: 0.1ì´ˆ ë¡œë”© + 2-3ì´ˆ ê²€ìƒ‰)
    """
    logger.info("ğŸ“š Fixed RAG ë¶„ì„ ì‹œì‘")
    logger.info(f"   Query: {query[:100]}...")
    
    # 1ï¸âƒ£ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (ìºì‹±ë¨)
    vectorstore = get_vectorstore()
    
    if vectorstore is None:
        return {
            "answer": "ë²¡í„° ì €ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'python scripts/build_vectorstore.py'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.",
            "sources": [],
            "loaded_documents": [],
            "error": True
        }
    
    # 2ï¸âƒ£ í•œê¸€ ì§ˆì˜ â†’ ì˜ì–´ ë³€í™˜
    if not query.isascii():
        logger.info("ğŸŒ í•œê¸€ ì§ˆì˜ë¥¼ ì˜ì–´ë¡œ ë³€í™˜ ì¤‘...")
        query_en = translate_query_to_english(query)
        logger.info(f"   ë²ˆì—­ ì™„ë£Œ â†’ {query_en[:80]}...")
    else:
        query_en = query
    
    # 3ï¸âƒ£ LLM ì´ˆê¸°í™”
    logger.info("   ğŸ¤– LLM ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    
    try:
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY
        )
        
        # 4ï¸âƒ£ í•œêµ­ì–´ ë‹µë³€ìš© í”„ë¡¬í”„íŠ¸
        prompt = PromptTemplate(
            template=(
                "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ë¶„ì„ì ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n"
                "ì§ˆë¬¸:\n{question}\n\n"
                "ì»¨í…ìŠ¤íŠ¸:\n{context}\n"
            ),
            input_variables=["question", "context"]
        )
        
        # 5ï¸âƒ£ Retriever ì¤€ë¹„
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # 6ï¸âƒ£ ë¬¸ì„œ í¬ë§·í„°
        def format_docs(docs):
            return "\n\n".join(d.page_content for d in docs)
        
        # 7ï¸âƒ£ LCEL ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸
        rag_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # 8ï¸âƒ£ ì˜ì–´ ì§ˆì˜ ì‹¤í–‰
        answer_ko = rag_chain.invoke(query_en)
        
        # 9ï¸âƒ£ ì¶œì²˜ ì •ë¦¬
        top_docs = retriever.invoke(query_en)[:3]
        sources = []
        for doc in top_docs:
            sources.append({
                "content": doc.page_content[:300],
                "page": doc.metadata.get("page", 0) + 1,
                "source": os.path.basename(doc.metadata.get("source", "unknown"))
            })
        
        logger.info("   âœ… RAG ë¶„ì„ ì™„ë£Œ")
        logger.info(f"      ë‹µë³€ ê¸¸ì´: {len(answer_ko)}ì")
        
        return {
            "answer": answer_ko,
            "sources": sources,
            "loaded_documents": ["OECD PDF", "IMF PDF"],
            "num_pages": "N/A (ì‚¬ì „ ì¸ë±ì‹±)",
            "num_chunks": "N/A (ì‚¬ì „ ì¸ë±ì‹±)",
            "error": False
        }
        
    except Exception as e:
        logger.error(f"   âŒ RAG ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return {
            "answer": f"RAG ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
            "sources": [],
            "loaded_documents": [],
            "error": True
        }