# tools/arxiv_tool.py
"""
arXiv ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬
"""
from langchain_core.tools import tool
from typing import List, Dict
import arxiv
from datetime import datetime
import time
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import settings
from utils.logger import logger

@tool
def search_arxiv_papers(keywords: List[str], max_results: int = 100) -> List[Dict]:
    """
    arXivì—ì„œ AI ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    Args:
        keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        max_results: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê²°ê³¼ ìˆ˜
    
    Returns:
        ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸ“„ arXiv ë…¼ë¬¸ ê²€ìƒ‰ ì‹œì‘ (í‚¤ì›Œë“œ: {len(keywords)}ê°œ)")
    
    papers = []
    
    # âœ… ë‚ ì§œë¥¼ date ê°ì²´ë¡œ ë³€í™˜
    start_date = datetime.strptime(settings.ANALYSIS["date_range"]["start"], "%Y-%m-%d").date()
    end_date = datetime.strptime(settings.ANALYSIS["date_range"]["end"], "%Y-%m-%d").date()
    
    for idx, keyword in enumerate(keywords, 1):
        try:
            logger.info(f"   [{idx}/{len(keywords)}] ê²€ìƒ‰ ì¤‘: '{keyword}'")
            
            # âœ… ê²€ìƒ‰ ì„¤ì •
            search = arxiv.Search(
                query=keyword,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            count = 0
            keyword_papers = []
            
            # âœ… ê²°ê³¼ ìˆœíšŒ (ì—ëŸ¬ í•¸ë“¤ë§ ì¶”ê°€)
            try:
                for result in search.results():
                    try:
                        pub_date = result.published.date()
                        
                        # ë‚ ì§œ í•„í„°ë§
                        if pub_date < start_date or pub_date > end_date:
                            continue
                        
                        # ë…¼ë¬¸ ì •ë³´ ì €ì¥
                        keyword_papers.append({
                            "id": result.entry_id.split('/')[-1],  # arXiv IDë§Œ ì¶”ì¶œ
                            "title": result.title.strip(),
                            "authors": [a.name for a in result.authors][:5],
                            "abstract": result.summary.strip()[:500],
                            "publish_date": pub_date.isoformat(),
                            "keywords": [keyword],
                            "url": result.entry_id,
                            "categories": result.categories if hasattr(result, 'categories') else []
                        })
                        count += 1
                        
                        # âœ… ìµœëŒ€ ê°œìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                        if count >= max_results:
                            break
                            
                    except Exception as e:
                        # ê°œë³„ ë…¼ë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨ëŠ” ê±´ë„ˆë›°ê¸°
                        continue
                
            except StopIteration:
                # ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ
                pass
            except arxiv.UnexpectedEmptyPageError:
                # ë¹ˆ í˜ì´ì§€ ì—ëŸ¬ (ë¬´ì‹œ)
                logger.warning(f"      âš ï¸ ë” ì´ìƒ ê²°ê³¼ ì—†ìŒ")
            except Exception as e:
                logger.warning(f"      âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸: {e}")
            
            # ìˆ˜ì§‘ëœ ë…¼ë¬¸ ì¶”ê°€
            papers.extend(keyword_papers)
            logger.info(f"      âœ“ {count}ê°œ ìˆ˜ì§‘")
            
            # âœ… Rate limit ë°©ì§€ (í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°)
            if idx < len(keywords):
                time.sleep(3)  # 3ì´ˆ ëŒ€ê¸°
            
        except Exception as e:
            logger.error(f"      âœ— '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue
    
    # âœ… ì¤‘ë³µ ì œê±° (ê°™ì€ ë…¼ë¬¸ì´ ì—¬ëŸ¬ í‚¤ì›Œë“œì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
    unique_papers = {}
    for paper in papers:
        paper_id = paper["id"]
        if paper_id not in unique_papers:
            unique_papers[paper_id] = paper
        else:
            # í‚¤ì›Œë“œ ë³‘í•©
            existing_keywords = unique_papers[paper_id]["keywords"]
            new_keywords = paper["keywords"]
            unique_papers[paper_id]["keywords"] = list(set(existing_keywords + new_keywords))
    
    final_papers = list(unique_papers.values())
    
    logger.info(f"âœ… arXiv ì´ {len(final_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±° í›„)\n")
    
    return final_papers